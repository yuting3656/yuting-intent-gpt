{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5423de5b-56f9-4129-9cb8-d80bfd0698d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "044f3309-2a20-4509-ab39-635440e5a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[text for text, label in train_data] ['Hi there!', 'Hello!', 'Goodbye!', 'See you later!', 'Thanks!', 'Thank you!']\n",
      "train_sequences [[2, 3], [4], [5], [6, 1, 7], [8], [9, 1]]\n",
      "Train on 6 samples\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 135ms/sample - loss: 1.3899 - accuracy: 0.1667\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 628us/sample - loss: 1.3763 - accuracy: 0.8333\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 804us/sample - loss: 1.3661 - accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 767us/sample - loss: 1.3566 - accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 717us/sample - loss: 1.3477 - accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 957us/sample - loss: 1.3387 - accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 831us/sample - loss: 1.3299 - accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 751us/sample - loss: 1.3208 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 2ms/sample - loss: 1.3115 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 776us/sample - loss: 1.3018 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 762us/sample - loss: 1.2916 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 593us/sample - loss: 1.2808 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 808us/sample - loss: 1.2695 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 1.2573 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 662us/sample - loss: 1.2444 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 709us/sample - loss: 1.2309 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 655us/sample - loss: 1.2166 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 658us/sample - loss: 1.2015 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 874us/sample - loss: 1.1856 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 992us/sample - loss: 1.1689 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 585us/sample - loss: 1.1511 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 840us/sample - loss: 1.1323 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 799us/sample - loss: 1.1125 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 627us/sample - loss: 1.0915 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 806us/sample - loss: 1.0691 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 918us/sample - loss: 1.0458 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 783us/sample - loss: 1.0215 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 922us/sample - loss: 0.9959 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 631us/sample - loss: 0.9692 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 772us/sample - loss: 0.9415 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 889us/sample - loss: 0.9129 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 631us/sample - loss: 0.8835 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 594us/sample - loss: 0.8535 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 582us/sample - loss: 0.8230 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 869us/sample - loss: 0.7916 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 682us/sample - loss: 0.7600 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 703us/sample - loss: 0.7276 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 943us/sample - loss: 0.6951 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 823us/sample - loss: 0.6623 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.6294 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 833us/sample - loss: 0.5970 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 892us/sample - loss: 0.5645 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 976us/sample - loss: 0.5327 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 764us/sample - loss: 0.5016 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.4707 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 674us/sample - loss: 0.4407 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 819us/sample - loss: 0.4116 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 716us/sample - loss: 0.3834 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 823us/sample - loss: 0.3561 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.3297 - accuracy: 1.0000\n",
      "4/1 [========================================================================================================================] - 0s 40ms/sample - loss: 0.9599 - accuracy: 0.7500\n",
      "Test Loss: 0.9598678946495056, Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Define the intent labels\n",
    "labels = [\"greeting\", \"goodbye\", \"thanks\", \"unknown\"]\n",
    "\n",
    "# Define the training data\n",
    "train_data = [\n",
    "    (\"Hi there!\", \"greeting\"),\n",
    "    (\"Hello!\", \"greeting\"),\n",
    "    (\"Goodbye!\", \"goodbye\"),\n",
    "    (\"See you later!\", \"goodbye\"),\n",
    "    (\"Thanks!\", \"thanks\"),\n",
    "    (\"Thank you!\", \"thanks\")\n",
    "]\n",
    "\n",
    "# Tokenize the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text for text, label in train_data])\n",
    "\n",
    "# Convert the training data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences([text for text, label in train_data])\n",
    "print(\"[text for text, label in train_data]\", [text for text, label in train_data])\n",
    "print(\"train_sequences\", train_sequences)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = max(len(seq) for seq in train_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert the labels to one-hot encodings\n",
    "label_encoder = {label: i for i, label in enumerate(labels)}\n",
    "train_labels = [label_encoder[label] for text, label in train_data]\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(labels))\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_length))\n",
    "model.add(layers.Conv1D(64, 5, activation='relu', padding=\"same\"))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(len(labels), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels, epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "test_data = [\n",
    "    (\"Hi there!\", \"greeting\"),\n",
    "    (\"Goodbye!\", \"goodbye\"),\n",
    "    (\"Thanks!\", \"thanks\"),\n",
    "    (\"What's the weather like today?\", \"unknown\")\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences([text for text, label in test_data])\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "test_labels = [label_encoder[label] for text, label in test_data]\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(labels))\n",
    "\n",
    "loss, accuracy = model.evaluate(test_sequences, test_labels)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5668bac-6fa3-4c80-a681-e2ec7ca7fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 3, 64)             640       \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 3, 64)             20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 23,396\n",
      "Trainable params: 23,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9261c6ad-e1bf-4178-a5ec-f6a766e38d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = \"i have no ideas \"\n",
    "sequence_input = tokenizer.texts_to_sequences([questions])\n",
    "padded_input = pad_sequences(sequence_input, maxlen=max_length , padding=\"post\")\n",
    "padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "27ab0e9d-4654-475e-924f-0e46b2e51fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(padded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fa27ec7e-9805-4186-ac0a-f33cf6c7601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40442273, 0.18461552, 0.35208997, 0.05887186]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f4cddb95-747f-46e0-992e-e2edc73282a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老賽！\n"
     ]
    }
   ],
   "source": [
    "predicted_probs = model.predict(padded_input)[0]\n",
    "predicted_label_index = np.argmax(output)\n",
    "\n",
    "\n",
    "if predicted_probs[predicted_label_index] < 0.7:\n",
    "    print(\"老賽！\")\n",
    "else:\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "    print(\"predicted_label\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20987a-eedd-41df-9b6a-8c36c0a508a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dzGPT",
   "language": "python",
   "name": "dzgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
